"""
Test cases for Deep Galerkin Methods (DGM) foundation.

This module tests the core DGM infrastructure including sampling strategies,
neural architectures, variance reduction techniques, and the MFG DGM solver.
"""

from unittest.mock import Mock, patch

import pytest

import numpy as np

from mfg_pde.alg.neural.dgm.base_dgm import DGMConfig, DGMResult
from mfg_pde.alg.neural.dgm.sampling import (
    MonteCarloSampler,
    QuasiMonteCarloSampler,
    adaptive_sampling,
)
from mfg_pde.alg.neural.dgm.variance_reduction import (
    ControlVariates,
    ImportanceSampling,
    adaptive_importance_distribution,
    create_control_variate_function,
)

# Skip PyTorch tests if not available
pytorch_available = True
try:
    import torch

    from mfg_pde.alg.neural.dgm.architectures import (
        DeepGalerkinNetwork,
        HighDimMLP,
        ResidualDGMNetwork,
    )
    from mfg_pde.alg.neural.dgm.mfg_dgm_solver import MFGDGMSolver
except ImportError:
    pytorch_available = False


class TestDGMConfig:
    """Test DGM configuration dataclass."""

    def test_default_config(self):
        """Test default configuration values."""
        config = DGMConfig()

        assert config.hidden_layers == [256, 256, 256, 256]
        assert config.activation == "tanh"
        assert config.learning_rate == 1e-3
        assert config.num_interior_points == 10000
        assert config.sampling_strategy == "monte_carlo"
        assert config.use_control_variates is True
        assert config.tolerance == 1e-5

    def test_custom_config(self):
        """Test custom configuration."""
        config = DGMConfig(
            hidden_layers=[128, 128],
            learning_rate=1e-4,
            num_interior_points=5000,
            sampling_strategy="quasi_monte_carlo",
        )

        assert config.hidden_layers == [128, 128]
        assert config.learning_rate == 1e-4
        assert config.num_interior_points == 5000
        assert config.sampling_strategy == "quasi_monte_carlo"


class TestDGMResult:
    """Test DGM result container."""

    def test_default_result(self):
        """Test default result values."""
        result = DGMResult()

        assert result.value_network is None
        assert result.density_network is None
        assert result.final_loss == np.inf
        assert result.converged is False
        assert result.num_epochs == 0
        assert result.dimension == 0


class TestMonteCarloSampler:
    """Test Monte Carlo sampling for high-dimensional domains."""

    def test_initialization(self):
        """Test sampler initialization."""
        domain_bounds = [(-1, 1), (-2, 2)]
        sampler = MonteCarloSampler(domain_bounds, dimension=2, seed=42)

        assert sampler.dimension == 2
        assert sampler.total_dimension == 3  # Including time
        assert len(sampler.domain_bounds) == 2

    def test_sample_interior(self):
        """Test interior domain sampling."""
        domain_bounds = [(-1, 1), (-1, 1)]
        sampler = MonteCarloSampler(domain_bounds, dimension=2, seed=42)

        points = sampler.sample_interior(100, time_bounds=(0, 1))

        assert points.shape == (100, 3)  # (t, x1, x2)
        assert np.all(points[:, 0] >= 0)
        assert np.all(points[:, 0] <= 1)
        assert np.all(points[:, 1] >= -1)
        assert np.all(points[:, 1] <= 1)
        assert np.all(points[:, 2] >= -1)
        assert np.all(points[:, 2] <= 1)

    def test_sample_boundary(self):
        """Test boundary sampling."""
        domain_bounds = [(-1, 1), (-1, 1)]
        sampler = MonteCarloSampler(domain_bounds, dimension=2, seed=42)

        boundary_points = sampler.sample_boundary(40, time_bounds=(0, 1))

        # Should have points on all 4 boundary faces (2 * dimension)
        assert boundary_points.shape[0] >= 40 - 4  # Allow for integer division
        assert boundary_points.shape[1] == 3

        # Check that at least some points are on boundaries
        on_boundary = (
            (np.abs(boundary_points[:, 1] + 1) < 1e-10)  # x1 = -1
            | (np.abs(boundary_points[:, 1] - 1) < 1e-10)  # x1 = 1
            | (np.abs(boundary_points[:, 2] + 1) < 1e-10)  # x2 = -1
            | (np.abs(boundary_points[:, 2] - 1) < 1e-10)  # x2 = 1
        )
        assert np.any(on_boundary)

    def test_sample_initial(self):
        """Test initial condition sampling."""
        domain_bounds = [(-1, 1), (-1, 1)]
        sampler = MonteCarloSampler(domain_bounds, dimension=2, seed=42)

        initial_points = sampler.sample_initial(50)

        assert initial_points.shape == (50, 2)  # Only spatial dimensions
        assert np.all(initial_points[:, 0] >= -1)
        assert np.all(initial_points[:, 0] <= 1)
        assert np.all(initial_points[:, 1] >= -1)
        assert np.all(initial_points[:, 1] <= 1)


class TestQuasiMonteCarloSampler:
    """Test Quasi-Monte Carlo sampling."""

    def test_initialization_with_scipy(self):
        """Test QMC initialization when SciPy is available."""
        domain_bounds = [(-1, 1), (-1, 1)]

        with patch("mfg_pde.alg.neural.dgm.sampling.qmc") as mock_qmc:
            # Mock successful import
            mock_sobol = Mock()
            mock_qmc.Sobol.return_value = mock_sobol

            sampler = QuasiMonteCarloSampler(domain_bounds, dimension=2, sequence_type="sobol")

            assert sampler.sequence_type == "sobol"
            assert sampler.scipy_available is True
            mock_qmc.Sobol.assert_called_once()

    def test_initialization_without_scipy(self):
        """Test QMC fallback when SciPy unavailable."""
        domain_bounds = [(-1, 1), (-1, 1)]

        with patch("mfg_pde.alg.neural.dgm.sampling.qmc", side_effect=ImportError):
            sampler = QuasiMonteCarloSampler(domain_bounds, dimension=2)

            assert sampler.scipy_available is False
            assert hasattr(sampler, "rng")

    def test_sample_interior_fallback(self):
        """Test interior sampling with Monte Carlo fallback."""
        domain_bounds = [(-1, 1), (-1, 1)]

        with patch("mfg_pde.alg.neural.dgm.sampling.qmc", side_effect=ImportError):
            sampler = QuasiMonteCarloSampler(domain_bounds, dimension=2)
            points = sampler.sample_interior(100, time_bounds=(0, 1))

            assert points.shape == (100, 3)
            assert np.all(points[:, 0] >= 0)
            assert np.all(points[:, 0] <= 1)


class TestAdaptiveSampling:
    """Test adaptive sampling based on residual analysis."""

    def test_adaptive_sampling_high_residuals(self):
        """Test adaptive sampling with high residuals."""
        # Create sample points
        current_points = np.random.uniform(-1, 1, (50, 3))
        domain_bounds = [(-1, 1), (-1, 1)]

        # Mock residual function with high residuals at specific points
        def residual_function(points):
            # High residuals for points with x1 > 0.5
            return np.where(points[:, 1] > 0.5, 2.0, 0.1)

        new_points = adaptive_sampling(
            residual_function, current_points, threshold=1.0, max_new_points=100, domain_bounds=domain_bounds
        )

        assert len(new_points) > 0
        assert new_points.shape[1] == 3  # Same dimension as input points

        # Check bounds are respected
        assert np.all(new_points[:, 1] >= -1)
        assert np.all(new_points[:, 1] <= 1)
        assert np.all(new_points[:, 2] >= -1)
        assert np.all(new_points[:, 2] <= 1)

    def test_adaptive_sampling_low_residuals(self):
        """Test adaptive sampling with low residuals."""
        current_points = np.random.uniform(-1, 1, (50, 3))
        domain_bounds = [(-1, 1), (-1, 1)]

        # Mock residual function with all low residuals
        def residual_function(points):
            return np.ones(len(points)) * 0.01

        new_points = adaptive_sampling(
            residual_function, current_points, threshold=1.0, max_new_points=100, domain_bounds=domain_bounds
        )

        assert len(new_points) == 0


class TestControlVariates:
    """Test control variates for variance reduction."""

    def test_initialization(self):
        """Test control variate initialization."""

        def control_func(points):
            return np.sum(points**2, axis=1)

        cv = ControlVariates(control_function=control_func)

        assert cv.control_function is not None
        assert cv.control_coefficient == 0.0
        assert cv.is_calibrated is False

    def test_calibration(self):
        """Test control variate calibration."""

        # Simple control function
        def control_func(points):
            return np.sum(points**2, axis=1)

        # Target function correlated with control
        def target_func(points):
            return 2 * np.sum(points**2, axis=1) + np.random.normal(0, 0.1, len(points))

        cv = ControlVariates(control_function=control_func)
        sample_points = np.random.uniform(-1, 1, (1000, 3))

        cv.calibrate(sample_points, target_func)

        assert cv.is_calibrated is True
        assert cv.control_coefficient != 0.0

    def test_apply_variance_reduction(self):
        """Test variance reduction application."""

        def control_func(points):
            return np.sum(points**2, axis=1)

        cv = ControlVariates(control_function=control_func)
        cv.control_coefficient = 0.5
        cv.is_calibrated = True

        sample_points = np.random.uniform(-1, 1, (100, 3))
        function_values = np.random.normal(0, 1, 100)

        reduced_values = cv.apply(sample_points, function_values)

        assert len(reduced_values) == len(function_values)
        assert not np.array_equal(reduced_values, function_values)  # Should be different


class TestImportanceSampling:
    """Test importance sampling for efficient Monte Carlo."""

    def test_initialization(self):
        """Test importance sampling initialization."""
        is_sampler = ImportanceSampling()

        assert is_sampler.importance_function is None
        assert is_sampler.is_setup is False

    def test_setup_from_residuals(self):
        """Test setup from residual analysis."""
        sample_points = np.random.uniform(-1, 1, (100, 3))
        residuals = np.random.uniform(0, 1, 100)

        is_sampler = ImportanceSampling()
        is_sampler.setup_from_residuals(sample_points, residuals)

        assert is_sampler.is_setup is True
        assert is_sampler.importance_function is not None

    def test_uniform_sampling_fallback(self):
        """Test fallback to uniform sampling."""
        is_sampler = ImportanceSampling()
        domain_bounds = [(-1, 1), (-1, 1)]
        time_bounds = (0, 1)

        points, weights = is_sampler.sample(100, domain_bounds, time_bounds)

        assert points.shape == (100, 3)
        assert len(weights) == 100
        assert np.allclose(weights, 1.0 / 100)  # Uniform weights


class TestControlVariateFunctions:
    """Test control variate function creation."""

    def test_linear_quadratic_control(self):
        """Test linear-quadratic control variate."""
        control_func = create_control_variate_function("linear_quadratic")

        assert control_func is not None

        # Test function evaluation
        points = np.array([[0.5, 1.0, -1.0], [0.0, 0.0, 0.0]])
        values = control_func(points)

        assert len(values) == 2
        assert not np.isnan(values).any()

    def test_crowd_dynamics_control(self):
        """Test crowd dynamics control variate."""
        control_func = create_control_variate_function("crowd_dynamics")

        assert control_func is not None

        points = np.array([[0.5, 1.0, -1.0], [0.0, 0.0, 0.0]])
        values = control_func(points)

        assert len(values) == 2
        assert not np.isnan(values).any()

    def test_unknown_problem_type(self):
        """Test unknown problem type returns None."""
        control_func = create_control_variate_function("unknown_type")

        assert control_func is None


class TestAdaptiveImportanceDistribution:
    """Test adaptive importance distribution creation."""

    def test_normal_residuals(self):
        """Test importance distribution with normal residuals."""
        points = np.random.uniform(-1, 1, (100, 3))
        residuals = np.random.uniform(0, 1, 100)

        importance_func = adaptive_importance_distribution(points, residuals)

        assert importance_func is not None

        # Test evaluation
        query_points = np.random.uniform(-1, 1, (50, 3))
        importance_values = importance_func(query_points)

        assert len(importance_values) == 50
        assert np.all(importance_values > 0)  # Should be positive

    def test_zero_residuals(self):
        """Test importance distribution with zero residuals."""
        points = np.random.uniform(-1, 1, (100, 3))
        residuals = np.zeros(100)

        importance_func = adaptive_importance_distribution(points, residuals)

        query_points = np.random.uniform(-1, 1, (50, 3))
        importance_values = importance_func(query_points)

        # Should return uniform importance
        assert np.allclose(importance_values, 1.0)


@pytest.mark.skipif(not pytorch_available, reason="PyTorch not available")
class TestDGMArchitectures:
    """Test neural network architectures for DGM."""

    def test_deep_galerkin_network(self):
        """Test Deep Galerkin network architecture."""
        input_dim = 5  # 4D space + 1D time
        network = DeepGalerkinNetwork(input_dim, hidden_layers=[128, 128])

        # Test forward pass
        x = torch.randn(32, input_dim)
        output = network(x)

        assert output.shape == (32, 1)  # Scalar output
        assert not torch.isnan(output).any()

    def test_high_dim_mlp(self):
        """Test high-dimensional MLP."""
        input_dim = 10  # 9D space + 1D time
        network = HighDimMLP(input_dim, base_width=64, num_layers=4)

        x = torch.randn(16, input_dim)
        output = network(x)

        assert output.shape == (16, 1)
        assert not torch.isnan(output).any()

    def test_residual_dgm_network(self):
        """Test residual DGM network."""
        input_dim = 6  # 5D space + 1D time
        network = ResidualDGMNetwork(input_dim, hidden_dim=128, num_residual_blocks=4)

        x = torch.randn(24, input_dim)
        output = network(x)

        assert output.shape == (24, 1)
        assert not torch.isnan(output).any()


@pytest.mark.skipif(not pytorch_available, reason="PyTorch not available")
class TestMFGDGMSolver:
    """Test MFG DGM solver implementation."""

    def test_solver_initialization(self):
        """Test solver initialization."""
        # Mock MFG problem
        mock_problem = Mock()
        mock_problem.dimension = 2
        mock_problem.domain = [(-1, 1), (-1, 1)]
        mock_problem.time_domain = (0, 1)

        config = DGMConfig(max_epochs=10, num_interior_points=100)
        solver = MFGDGMSolver(mock_problem, config)

        assert solver.dimension == 2
        assert solver.config.max_epochs == 10
        assert hasattr(solver, "value_network")
        assert hasattr(solver, "density_network")

    def test_sampling_setup(self):
        """Test sampling strategy setup."""
        mock_problem = Mock()
        mock_problem.dimension = 2
        mock_problem.domain = [(-1, 1), (-1, 1)]

        config = DGMConfig(sampling_strategy="monte_carlo")
        solver = MFGDGMSolver(mock_problem, config)

        # Test interior sampling
        interior_points = solver._sample_interior_points(50)
        assert interior_points.shape == (50, 3)  # 2D + time

        # Test boundary sampling
        boundary_points = solver._sample_boundary_points(20)
        assert boundary_points.shape[1] == 3

        # Test initial sampling
        initial_points = solver._sample_initial_points(30)
        assert initial_points.shape == (30, 2)  # Only spatial


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
