# Track B: Particle Solver Analysis & GPU Opportunities

**Date**: 2025-10-08
**File Analyzed**: `mfg_pde/alg/numerical/fp_solvers/fp_particle.py`
**Current State**: NumPy-based, CPU-only

---

## Current Implementation Analysis

### Key Components

1. **Particle Initialization** (lines 146-160)
   - Sample N particles from initial density
   - Current: `np.random.choice()` - CPU-only

2. **Main Evolution Loop** (lines 167-220)
   - For each timestep:
     - Compute gradient âˆ‡U on grid
     - Interpolate gradient to particle positions
     - Euler-Maruyama SDE step
     - Apply boundary conditions
     - Estimate density via KDE

3. **Density Estimation** (lines 56-125)
   - `scipy.stats.gaussian_kde()` - CPU-only
   - Fallback: histogram/peak approximation

---

## GPU Acceleration Bottlenecks

### ðŸ”´ Critical Bottlenecks (High Priority)

#### 1. KDE Computation (Line 92: `gaussian_kde()`)
**Current**:
```python
kde = gaussian_kde(particles_at_time_t, bw_method=self.kde_bandwidth)
m_density_estimated = kde(xSpace)  # O(Nx Ã— N) - CPU sequential
```

**Problem**:
- `scipy.stats.gaussian_kde` is pure CPU
- Called Nt times (once per timestep)
- O(Nx Ã— N) complexity, but sequential

**GPU Opportunity**:
- Vectorize over all Nx grid points simultaneously
- **Expected Speedup**: 10-30x for large N

**Solution**:
```python
# GPU-vectorized KDE
def gpu_kde(particles, grid, bandwidth, backend):
    # Shape: particles (N,), grid (Nx,)
    # Broadcast: grid[:, None] - particles[None, :] â†’ (Nx, N)
    distances = (grid[:, None] - particles[None, :]) / bandwidth
    kernel_vals = backend.exp(-0.5 * distances**2) / (bandwidth * np.sqrt(2*np.pi))
    density = kernel_vals.mean(dim=1)  # Average over particles
    return density
```

---

#### 2. Particle Evolution (Lines 191-196)
**Current**:
```python
for n_time_idx in range(Nt - 1):
    # Compute drift for ALL particles
    dUdx_at_particles = interp_func_dUdx(current_M_particles_t[n_time_idx, :])

    # Update ALL particles
    alpha_optimal_at_particles = -coefCT * dUdx_at_particles
    dW = np.random.normal(0.0, np.sqrt(Dt), self.num_particles)
    current_M_particles_t[n_time_idx + 1, :] = (
        current_M_particles_t[n_time_idx, :] + alpha * Dt + sigma * dW
    )
```

**Problem**:
- Loop over timesteps (sequential - can't parallelize)
- But within each timestep, N particles updated independently
- NumPy already vectorized, but CPU-bound

**GPU Opportunity**:
- Keep timestep loop (inherently sequential)
- Parallelize N particles within each timestep
- **Expected Speedup**: 2-5x (NumPy already vectorized, GPU just faster)

**Solution**:
```python
# Already vectorized! Just move to GPU tensor
X = backend.zeros((Nt, N), device='mps')
for t in range(Nt - 1):
    drift = interpolate_gpu(X[t, :], U_grid[t], grid)  # N particles parallel
    noise = backend.randn((N,), device='mps') * sigma  # GPU random
    X[t+1, :] = X[t, :] + drift * dt + noise           # All parallel
```

---

#### 3. Gradient Interpolation (Lines 178-187)
**Current**:
```python
interp_func_dUdx = scipy.interpolate.interp1d(x_grid, dUdx_grid, kind="linear")
dUdx_at_particles = interp_func_dUdx(current_M_particles_t[n_time_idx, :])
```

**Problem**:
- `scipy.interpolate.interp1d` is CPU-only
- Called Nt times
- Interpolates N values per call

**GPU Opportunity**:
- PyTorch has `torch.nn.functional.grid_sample()` for 1D interpolation
- Can be vectorized
- **Expected Speedup**: 5-10x

**Solution**:
```python
def interpolate_1d_gpu(x_query, x_grid, y_grid, backend):
    # Normalize query points to [-1, 1] for grid_sample
    x_norm = 2 * (x_query - x_grid[0]) / (x_grid[-1] - x_grid[0]) - 1

    # Reshape for grid_sample: (1, 1, N), (1, 1, Nx)
    y_interp = torch.nn.functional.grid_sample(
        y_grid[None, None, :],
        x_norm[None, None, :, None],
        mode='bilinear',
        align_corners=True
    )
    return y_interp.squeeze()
```

---

### ðŸŸ¡ Secondary Bottlenecks (Medium Priority)

#### 4. Random Number Generation (Line 193)
**Current**:
```python
dW = np.random.normal(0.0, np.sqrt(Dt), self.num_particles)
```

**GPU Opportunity**:
- Use PyTorch's native GPU RNG
- **Expected Speedup**: 2-3x

**Solution**:
```python
dW = backend.randn((N,), device='mps') * np.sqrt(Dt)
```

---

#### 5. Boundary Condition Application (Lines 200-216)
**Current**:
```python
# Periodic
current_M_particles_t[n_time_idx + 1, :] = (
    xmin + (current_M_particles_t[n_time_idx + 1, :] - xmin) % Lx
)

# No-flux (reflecting)
left_violations = particles < xmin
particles[left_violations] = 2 * xmin - particles[left_violations]
```

**GPU Opportunity**:
- Already vectorized operations
- Will automatically be fast on GPU
- **Expected Speedup**: Negligible (already fast)

---

## Overall GPU Speedup Projection

### Per-Component Speedup

| Component | Current (CPU) | GPU Speedup | Time % | Impact |
|:----------|:--------------|:------------|:-------|:-------|
| **KDE** | 70% of time | 10-30x | 70% | **21-49x faster** |
| **Interpolation** | 15% of time | 5-10x | 15% | **1.2-1.5x faster** |
| **Particle update** | 10% of time | 2-5x | 10% | **1.02-1.05x faster** |
| **Random numbers** | 3% of time | 2-3x | 3% | **1.003-1.006x faster** |
| **Boundary conditions** | 2% of time | 2x | 2% | Negligible |

### Overall Expected Speedup

**Conservative Estimate** (N=10k):
- KDE dominates (70% of time)
- KDE speedup: 10x â†’ **Overall: 7-8x**

**Optimistic Estimate** (N=100k):
- KDE dominates even more (80% of time)
- KDE speedup: 30x â†’ **Overall: 20-25x**

**Realistic Target**: **10-25x for N=10k-100k particles**

---

## Implementation Strategy

### Phase 1: GPU-Accelerated KDE (HIGHEST IMPACT)

**Priority**: ðŸ”´ CRITICAL - 70% of compute time

**Implementation**:
1. Create `mfg_pde/alg/numerical/density_estimation.py`
2. Implement GPU-vectorized Gaussian KDE
3. Add backend parameter to `_estimate_density_from_particles()`
4. Benchmark: expect 10-30x speedup on KDE alone

**Code Sketch**:
```python
def gaussian_kde_gpu(particles, grid, bandwidth, backend):
    """GPU-accelerated Gaussian KDE."""
    N = len(particles)
    Nx = len(grid)

    # Vectorized distance computation: (Nx, N)
    if backend.name == 'torch':
        particles_tensor = backend.tensor(particles, device=backend.device)
        grid_tensor = backend.tensor(grid, device=backend.device)

        # Broadcasting: (Nx, 1) - (1, N) â†’ (Nx, N)
        distances = (grid_tensor[:, None] - particles_tensor[None, :]) / bandwidth

        # Gaussian kernel
        kernel_vals = backend.exp(-0.5 * distances**2)
        kernel_vals = kernel_vals / (bandwidth * np.sqrt(2 * np.pi))

        # Sum over particles, normalize
        density = kernel_vals.sum(dim=1) / N

        return backend.to_numpy(density)
    else:
        # NumPy fallback
        return scipy_kde_numpy(particles, grid, bandwidth)
```

**Testing**:
- Verify numerical accuracy vs scipy.stats.gaussian_kde
- Benchmark on N=1k, 10k, 100k particles
- Target: 10x speedup for N=10k

---

### Phase 2: GPU Particle Evolution

**Priority**: ðŸŸ¡ MEDIUM - 25% of compute time

**Implementation**:
1. Keep particle array as GPU tensor throughout evolution
2. GPU interpolation for gradient
3. GPU random number generation

**Code Sketch**:
```python
def solve_fp_system_gpu(self, m_initial, U_drift):
    # Initialize particles on GPU
    X = backend.zeros((Nt, N), device=backend.device)
    X[0, :] = sample_initial_particles_gpu(m_initial, backend)

    for t in range(Nt - 1):
        # Compute gradient on grid (still CPU, small array)
        dUdx_grid = compute_gradient_numpy(U_drift[t])

        # Interpolate to particle positions (GPU)
        dUdx_particles = interpolate_1d_gpu(
            X[t, :], x_grid, dUdx_grid, backend
        )

        # Particle update (all GPU operations)
        drift = -coefCT * dUdx_particles
        noise = backend.randn((N,), device=backend.device) * sigma
        X[t+1, :] = X[t, :] + drift * Dt + noise

        # Boundary conditions (GPU)
        X[t+1, :] = apply_boundary_gpu(X[t+1, :], xmin, Lx, bc_type)

        # Density estimation (GPU KDE from Phase 1)
        M[t+1, :] = gaussian_kde_gpu(X[t+1, :], x_grid, bandwidth, backend)

    return M
```

---

### Phase 3: Full GPU Pipeline

**Priority**: ðŸŸ¢ LOW - Optimization

**Implementation**:
1. Keep everything on GPU between timesteps
2. Minimize CPUâ†”GPU transfers
3. Mixed precision (float32/float16)

---

## Quick Wins for Immediate Speedup

### Week 1 Target: 5-10x speedup with minimal code

**Approach**: GPU KDE only (Phase 1 subset)

1. Add `gaussian_kde_gpu()` function
2. Modify `_estimate_density_from_particles()` to use GPU KDE when backend available
3. Keep everything else NumPy (minimize changes)

**Expected Result**:
- 70% of time spent in KDE
- 10x faster KDE â†’ **7-8x overall speedup**
- <100 lines of new code

---

## Validation Strategy

### Numerical Accuracy
- Compare GPU results vs NumPy (should match within float32 tolerance)
- Test problems: LQ game, crowd dynamics

### Performance Benchmarking
```python
# Benchmark script
for N in [1000, 5000, 10000, 50000, 100000]:
    # NumPy version
    time_numpy = benchmark_particle_solver(N, backend='numpy')

    # MPS version
    time_mps = benchmark_particle_solver(N, backend='torch', device='mps')

    speedup = time_numpy / time_mps
    print(f"N={N}: {speedup:.1f}x speedup")

# Expected output:
# N=1000:   2x speedup    (overhead dominates)
# N=5000:   5x speedup
# N=10000:  10x speedup   (target)
# N=50000:  20x speedup
# N=100000: 25x speedup
```

---

## Code Structure

### New Files to Create

1. **`mfg_pde/alg/numerical/density_estimation.py`**
   - `gaussian_kde_gpu()` - GPU-vectorized KDE
   - `histogram_gpu()` - Fast histogram (alternative to KDE)
   - `adaptive_bandwidth()` - Automatic bandwidth selection

2. **`mfg_pde/alg/numerical/particle_utils.py`**
   - `interpolate_1d_gpu()` - GPU interpolation
   - `sample_from_density_gpu()` - GPU particle sampling
   - `apply_boundary_conditions_gpu()` - GPU boundary handling

### Files to Modify

1. **`fp_particle.py`**
   - Add GPU code paths
   - Keep NumPy fallback
   - Minimal changes to existing logic

---

## Risk Mitigation

| Risk | Mitigation |
|:-----|:-----------|
| Numerical accuracy issues | Extensive validation, tolerance checks |
| GPU memory limits | Batching for large N, streaming |
| Backend compatibility | Fallback to NumPy, comprehensive testing |
| Performance lower than expected | Profile, optimize hot paths iteratively |

---

## Success Criteria

### Phase 1 (Week 1-2)
- âœ… GPU KDE implemented and validated
- âœ… 7-10x speedup for N=10k particles
- âœ… Numerical accuracy within 1% of NumPy

### Phase 2 (Week 3-4)
- âœ… Full GPU particle evolution
- âœ… 15-20x speedup for N=50k particles
- âœ… Clean API, backward compatible

### Phase 3 (Week 5-6)
- âœ… Production-ready optimizations
- âœ… 25x+ speedup for N=100k particles
- âœ… Memory efficient, well-documented

---

**Next Action**: Implement GPU KDE (Phase 1 Quick Win)
