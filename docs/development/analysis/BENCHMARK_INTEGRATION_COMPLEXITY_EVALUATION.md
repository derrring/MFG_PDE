# Benchmark Acceleration Integration: Complexity Evaluation

**Date**: 2025-10-08
**Context**: Evaluating if GPU acceleration makes algorithm files too complicated
**Question**: Does benchmark integration compromise code clarity and maintainability?

---

## Executive Summary

**Verdict**: ‚ö†Ô∏è **Moderate Complexity Increase with Clear Separation**

The GPU acceleration integration in `fp_particle.py` has increased complexity moderately (~2.5x file size: 150‚Üí393 lines), but maintains **good separation of concerns** with distinct CPU and GPU pipelines. The code is production-ready but could benefit from **strategy pattern refactoring** to improve clarity.

**Key Findings**:
- ‚úÖ **Clean separation**: `_solve_fp_system_cpu()` and `_solve_fp_system_gpu()` are independent
- ‚úÖ **Modular utilities**: GPU operations extracted to separate files (`density_estimation.py`, `particle_utils.py`)
- ‚ö†Ô∏è **Hard-coded dispatch**: Simple if-else in `solve_fp_system()` (elegant solution exists but not integrated)
- ‚ö†Ô∏è **Code duplication**: ~40% overlap between CPU and GPU pipelines
- ‚ùå **Outdated comments**: "Expected speedup: 5-10x" should be "1.5-2x on MPS, 3-5x on CUDA"

---

## Complexity Metrics

### File Size Growth

| File | Lines of Code | Purpose | Growth |
|:-----|:--------------|:--------|:-------|
| **fp_particle.py** | 393 | Main solver | +163% (150‚Üí393) |
| **density_estimation.py** | 326 | KDE implementations | NEW (GPU support) |
| **particle_utils.py** | 354 | GPU utilities | NEW (GPU support) |
| **Total Algorithm Code** | 1073 | Particle solver ecosystem | +616% (150‚Üí1073) |

**Interpretation**:
- **Main solver**: 2.6x larger (reasonable for dual CPU/GPU pipelines)
- **Supporting code**: 680 lines in separate modules (good modularity)
- **Total growth**: Large in absolute terms, but well-organized

### Code Organization Analysis

**Before GPU Acceleration** (Phase 0):
```
fp_particle.py (150 lines)
  ‚îú‚îÄ‚îÄ __init__()
  ‚îú‚îÄ‚îÄ _estimate_density_from_particles()  # scipy KDE
  ‚îî‚îÄ‚îÄ solve_fp_system()                   # CPU only
```

**After GPU Acceleration** (Phase 2.1):
```
fp_particle.py (393 lines)
  ‚îú‚îÄ‚îÄ __init__()                          # Backend setup
  ‚îú‚îÄ‚îÄ _estimate_density_from_particles()  # Legacy wrapper (CPU)
  ‚îú‚îÄ‚îÄ solve_fp_system()                   # Dispatcher (if/else)
  ‚îú‚îÄ‚îÄ _solve_fp_system_cpu()             # CPU pipeline (98 lines)
  ‚îî‚îÄ‚îÄ _solve_fp_system_gpu()             # GPU pipeline (143 lines)

density_estimation.py (326 lines)
  ‚îú‚îÄ‚îÄ gaussian_kde_gpu()                  # External KDE (Phase 1)
  ‚îú‚îÄ‚îÄ gaussian_kde_gpu_internal()         # Internal KDE (Phase 2.1)
  ‚îú‚îÄ‚îÄ adaptive_bandwidth_selection()      # Bandwidth helpers
  ‚îî‚îÄ‚îÄ histogram_density_estimation()      # Alternative method

particle_utils.py (354 lines)
  ‚îú‚îÄ‚îÄ interpolate_1d_gpu()                # GPU interpolation
  ‚îú‚îÄ‚îÄ apply_boundary_conditions_gpu()     # GPU boundaries
  ‚îú‚îÄ‚îÄ sample_from_density_gpu()           # GPU sampling
  ‚îî‚îÄ‚îÄ CPU fallback implementations
```

---

## Complexity Assessment by Component

### 1. Main Solver (`fp_particle.py`)

#### Initialization Complexity: ‚≠ê‚≠ê (Low-Medium)

**Lines 26-54** (29 lines):
```python
def __init__(self, problem, num_particles, kde_bandwidth,
             normalize_kde_output, boundary_conditions, backend):
    # Backend setup (7 lines)
    from mfg_pde.backends import create_backend
    if backend is not None:
        self.backend = create_backend(backend)
    else:
        self.backend = create_backend("numpy")

    # Boundary conditions (4 lines)
    if boundary_conditions is None:
        self.boundary_conditions = BoundaryConditions(type="periodic")
    else:
        self.boundary_conditions = boundary_conditions
```

**Complexity Assessment**:
- ‚úÖ **Clear intent**: Backend and boundary conditions are explicit
- ‚úÖ **Sensible defaults**: NumPy fallback, periodic boundaries
- ‚ö†Ô∏è **Minor overhead**: 7 lines for backend setup (reasonable)
- **Verdict**: Acceptable complexity increase

#### Dispatch Logic: ‚≠ê (Low Complexity)

**Lines 134-150** (17 lines):
```python
def solve_fp_system(self, m_initial_condition, U_solution_for_drift):
    """
    Pipeline Selection Strategy (Track B Phase 2):
    - If backend available: Full GPU pipeline
    - If backend is None: CPU pipeline
    """
    if self.backend is not None:
        return self._solve_fp_system_gpu(m_initial_condition, U_solution_for_drift)
    else:
        return self._solve_fp_system_cpu(m_initial_condition, U_solution_for_drift)
```

**Complexity Assessment**:
- ‚úÖ **Very simple**: Single if-else, clear logic
- ‚úÖ **Well-documented**: Explains strategy
- ‚ö†Ô∏è **Hard-coded**: Violates elegant design (strategy pattern exists but not integrated)
- üîÑ **Refactoring ready**: Could use `StrategySelector` (see below)
- **Verdict**: Low complexity, but hard-coded (technical debt)

#### CPU Pipeline: ‚≠ê‚≠ê (Low-Medium Complexity)

**Lines 152-248** (98 lines):
```python
def _solve_fp_system_cpu(self, m_initial_condition, U_solution_for_drift):
    """CPU pipeline - existing NumPy implementation."""
    # [Standard particle solver code - mostly unchanged]
    # - Initialize particles
    # - Evolution loop with scipy interpolation
    # - Boundary conditions
    # - KDE using scipy or gaussian_kde_gpu()
```

**Complexity Assessment**:
- ‚úÖ **Familiar structure**: Standard particle solver
- ‚úÖ **Isolated**: Doesn't interfere with GPU code
- ‚úÖ **Optional GPU KDE**: Can use `gaussian_kde_gpu()` even on CPU pipeline
- **Verdict**: Low complexity, mostly original code

#### GPU Pipeline: ‚≠ê‚≠ê‚≠ê (Medium Complexity)

**Lines 250-392** (143 lines):
```python
def _solve_fp_system_gpu(self, m_initial_condition, U_solution_for_drift):
    """GPU pipeline - full particle evolution on GPU."""

    # Phase 2.1: Full GPU acceleration including internal KDE
    from mfg_pde.alg.numerical.density_estimation import gaussian_kde_gpu_internal
    from mfg_pde.alg.numerical.particle_utils import (
        apply_boundary_conditions_gpu,
        interpolate_1d_gpu,
        sample_from_density_gpu,
    )

    # Get backend array module
    xp = self.backend.array_module

    # Transfer inputs to GPU ONCE
    x_grid_gpu = self.backend.from_numpy(x_grid)
    U_drift_gpu = self.backend.from_numpy(U_solution_for_drift)

    # Evolution loop - ALL GPU
    for t in range(Nt - 1):
        # Compute gradient on GPU
        # Interpolate to particles on GPU
        # Euler-Maruyama update on GPU
        # Apply boundaries on GPU
        # KDE on GPU (internal, no transfers)
        # Normalize on GPU

    # Transfer results to CPU ONCE at end
    return self.backend.to_numpy(M_density_gpu)
```

**Complexity Assessment**:
- ‚ö†Ô∏è **Moderate complexity**: ~40% overlap with CPU pipeline (code duplication)
- ‚úÖ **Clear structure**: Imports ‚Üí setup ‚Üí loop ‚Üí return
- ‚úÖ **Explicit transfers**: "ONCE at start" and "ONCE at end" (clear intent)
- ‚ö†Ô∏è **Backend-specific code**: `xp = backend.array_module`, `hasattr(tensor, 'item')`
- ‚ö†Ô∏è **Outdated comment**: "Expected speedup: 5-10x" should be corrected
- **Verdict**: Medium complexity, acceptable for GPU code

---

## Code Quality Issues

### Issue 1: Code Duplication (~40% overlap)

**Duplicated Logic Between CPU and GPU Pipelines**:

| Operation | CPU Implementation | GPU Implementation | Duplication? |
|:----------|:-------------------|:-------------------|:-------------|
| **Initialization** | `np.zeros((Nt, Nx))` | `backend.zeros((Nt, Nx))` | ‚úÖ Same logic |
| **Particle sampling** | `np.random.choice()` | `sample_from_density_gpu()` | ‚ö†Ô∏è Different API |
| **Gradient computation** | `np.roll()` | `xp.roll()` | ‚úÖ Same logic |
| **Interpolation** | `scipy.interp1d()` | `interpolate_1d_gpu()` | ‚ö†Ô∏è Different API |
| **Euler-Maruyama** | `X + drift*dt + noise` | `X + drift*dt + noise` | ‚úÖ Same logic |
| **Boundary conditions** | Manual if-else | `apply_boundary_conditions_gpu()` | ‚ö†Ô∏è Different API |
| **KDE** | `gaussian_kde()` | `gaussian_kde_gpu_internal()` | ‚ö†Ô∏è Different API |
| **Normalization** | `M / np.sum(M*Dx)` | `M / xp.sum(M*Dx)` | ‚úÖ Same logic |

**Assessment**:
- ‚úÖ **Algorithmic logic identical**: Same physics, different APIs
- ‚ö†Ô∏è **API differences necessary**: GPU requires tensor operations
- ‚ùå **~60 lines duplicated**: Loop structure, gradient, normalization
- üîÑ **Refactoring opportunity**: Extract shared logic to helper functions

**Example Refactoring**:
```python
def _compute_gradient(self, U_array, Dx, backend=None):
    """Compute gradient using backend-agnostic operations."""
    if backend is None:
        return (np.roll(U_array, -1) - np.roll(U_array, 1)) / (2 * Dx)
    else:
        xp = backend.array_module
        return (xp.roll(U_array, -1) - xp.roll(U_array, 1)) / (2 * Dx)
```

### Issue 2: Outdated Comments

**Line 257** (GPU pipeline docstring):
```python
"""
Expected speedup: 5-10x vs CPU for N=10k-100k particles
"""
```

**Should be**:
```python
"""
Expected speedup: 1.5-2x on MPS, 3-5x on CUDA for N‚â•50k particles
(See docs/development/TRACK_B_GPU_ACCELERATION_COMPLETE.md)
"""
```

### Issue 3: Hard-Coded Backend Dispatch

**Line 145** (`solve_fp_system`):
```python
if self.backend is not None:
    return self._solve_fp_system_gpu(...)
else:
    return self._solve_fp_system_cpu(...)
```

**Issue**: Violates separation of concerns (solver knows about backends)

**Elegant Solution** (already designed, not integrated):
```python
# Use StrategySelector for intelligent dispatch
from mfg_pde.backends.strategies import StrategySelector

def __init__(self, problem, backend, ...):
    self.backend = backend
    self.strategy_selector = StrategySelector()

def solve_fp_system(self, m_initial_condition, U_solution_for_drift):
    # Auto-select optimal strategy based on backend capabilities
    strategy = self.strategy_selector.select_strategy(
        self.backend,
        problem_size=(self.num_particles, self.problem.Nx, self.problem.Nt)
    )
    return strategy.solve(m_initial_condition, U_solution_for_drift, ...)
```

**Benefit**: Solver agnostic to backends, extensible for future strategies

---

## Supporting Files Complexity

### `density_estimation.py` (326 lines)

**Structure**:
```
gaussian_kde_gpu()              # 119 lines (external KDE, Phase 1)
gaussian_kde_gpu_internal()     # 68 lines (internal KDE, Phase 2.1)
adaptive_bandwidth_selection()  # 58 lines (bandwidth helpers)
histogram_density_estimation()  # 81 lines (alternative method)
```

**Complexity Assessment**:
- ‚úÖ **Well-focused**: Single responsibility (density estimation)
- ‚úÖ **Comprehensive docstrings**: Mathematical formulation, references
- ‚úÖ **Backward compatible**: Doesn't break existing CPU code
- ‚ö†Ô∏è **Two KDE versions**: External and internal (necessary for optimization)
- **Verdict**: ‚≠ê‚≠ê Low-medium complexity, well-organized

### `particle_utils.py` (354 lines)

**Structure**:
```
interpolate_1d_gpu()                 # GPU interpolation
apply_boundary_conditions_gpu()      # GPU boundary handling
sample_from_density_gpu()            # GPU sampling
_linspace_backend()                  # Backend-agnostic helpers
```

**Complexity Assessment**:
- ‚úÖ **Utility module**: Doesn't complicate main solver
- ‚úÖ **GPU-specific**: Clear purpose (GPU operations)
- ‚úÖ **Fallback logic**: Handles backend failures gracefully
- **Verdict**: ‚≠ê‚≠ê Low-medium complexity, appropriate for utilities

---

## Comparison: Before vs After

### Conceptual Complexity

**Before GPU Acceleration**:
```
User calls: solver.solve_fp_system(m0, U)
  ‚Üí Single code path (CPU only)
  ‚Üí scipy.stats.gaussian_kde()
  ‚Üí scipy.interpolate.interp1d()
```

**Conceptual steps**: 1 (call solver)

**After GPU Acceleration**:
```
User calls: solver.solve_fp_system(m0, U)
  ‚Üí Dispatcher checks backend
    ‚Üí CPU path: scipy KDE, scipy interpolation
    ‚Üí GPU path: gaussian_kde_gpu_internal(), interpolate_1d_gpu()
```

**Conceptual steps**: 2 (dispatcher + path selection)

**Complexity increase**: 2x (reasonable)

### User Experience Complexity

**Before**:
```python
solver = FPParticleSolver(problem, num_particles=5000)
M = solver.solve_fp_system(m0, U)
```

**After** (CPU - same interface):
```python
solver = FPParticleSolver(problem, num_particles=5000)
M = solver.solve_fp_system(m0, U)
```

**After** (GPU - one parameter change):
```python
solver = FPParticleSolver(problem, num_particles=5000, backend="torch")
M = solver.solve_fp_system(m0, U)
```

**User complexity increase**: 0 for CPU, 1 parameter for GPU (‚úÖ **minimal**)

### Maintenance Complexity

**Code paths to maintain**:
- **Before**: 1 (CPU only)
- **After**: 2 (CPU + GPU)

**Test coverage required**:
- **Before**: Unit tests for CPU
- **After**: Unit tests for CPU + GPU + integration tests

**Dependency complexity**:
- **Before**: NumPy, SciPy
- **After**: NumPy, SciPy + PyTorch (optional)

**Verdict**: ‚ö†Ô∏è **2x maintenance burden** (reasonable for 1.5-2x speedup on large problems)

---

## Recommendations

### Priority 1: Fix Outdated Comments ‚úÖ (Quick Fix)

**Action**: Update GPU pipeline docstring
**Effort**: 5 minutes
**Benefit**: Accurate user expectations

```python
# fp_particle.py line 257
"""
GPU pipeline - full particle evolution on GPU.

Track B Phase 2.1: Full GPU acceleration including internal KDE.
Eliminates all GPU‚ÜîCPU transfers during evolution loop.

Expected speedup:
  - Apple Silicon MPS: 1.5-2x for N‚â•50k particles
  - NVIDIA CUDA: 3-5x (estimated, not tested)
  - See docs/development/TRACK_B_GPU_ACCELERATION_COMPLETE.md
"""
```

### Priority 2: Extract Shared Helper Functions ‚ö†Ô∏è (Medium Refactor)

**Action**: Reduce code duplication by extracting shared logic
**Effort**: 2-4 hours
**Benefit**: ~60 lines reduction, easier maintenance

**Example**:
```python
def _compute_gradient(self, U_array, Dx):
    """Backend-agnostic gradient computation."""
    if hasattr(self.backend, 'array_module'):
        xp = self.backend.array_module
    else:
        xp = np
    return (xp.roll(U_array, -1) - xp.roll(U_array, 1)) / (2 * Dx)

def _normalize_density(self, M_array, Dx):
    """Backend-agnostic normalization."""
    if hasattr(self.backend, 'array_module'):
        xp = self.backend.array_module
        mass = xp.sum(M_array) * Dx
        mass_val = mass.item() if hasattr(mass, 'item') else float(mass)
    else:
        mass_val = float(np.sum(M_array) * Dx)

    return M_array / mass_val if mass_val > 1e-9 else M_array * 0
```

**Files to refactor**:
- `fp_particle.py`: Extract `_compute_gradient()`, `_normalize_density()`
- Reduce GPU pipeline from 143 ‚Üí ~110 lines
- Reduce CPU pipeline from 98 ‚Üí ~80 lines

### Priority 3: Integrate StrategySelector üîÑ (Major Refactor)

**Action**: Replace hard-coded if-else with strategy pattern
**Effort**: 6-8 hours
**Benefit**: Elegant architecture, extensible for JAX/custom kernels

**Current** (hard-coded):
```python
def solve_fp_system(self, m_initial_condition, U_solution_for_drift):
    if self.backend is not None:
        return self._solve_fp_system_gpu(...)
    else:
        return self._solve_fp_system_cpu(...)
```

**Proposed** (elegant):
```python
def solve_fp_system(self, m_initial_condition, U_solution_for_drift):
    strategy = self.strategy_selector.select_strategy(
        self.backend,
        problem_size=(self.num_particles, self.problem.Nx, self.problem.Nt)
    )
    return strategy.solve(
        m_initial_condition, U_solution_for_drift,
        self.problem, self.num_particles, self.kde_bandwidth,
        self.normalize_kde_output, self.boundary_conditions, self.backend
    )
```

**Benefits**:
- ‚úÖ Solver agnostic to backend implementation
- ‚úÖ Automatic strategy selection based on problem size
- ‚úÖ Extensible for future backends (JAX, custom CUDA)
- ‚úÖ Testable strategy logic independently

**Implementation plan**:
1. Extract CPU logic to `CPUParticleStrategy.solve()`
2. Extract GPU logic to `GPUParticleStrategy.solve()`
3. Pass static methods to strategies (or extract to module-level functions)
4. Integrate `StrategySelector` in `__init__()`
5. Update tests for strategy-based dispatch

### Priority 4: Modularize GPU Pipeline (Optional)

**Action**: Split GPU pipeline into smaller methods
**Effort**: 3-4 hours
**Benefit**: Easier to test and understand

**Current** (monolithic 143-line method):
```python
def _solve_fp_system_gpu(self, ...):
    # Setup (20 lines)
    # Initial sampling (15 lines)
    # Bandwidth calculation (10 lines)
    # Evolution loop (90 lines)
    # Return (8 lines)
```

**Proposed** (modular):
```python
def _solve_fp_system_gpu(self, ...):
    x_grid_gpu, U_drift_gpu = self._transfer_inputs_to_gpu(...)
    X_particles_gpu = self._initialize_particles_gpu(m_initial_condition, ...)
    bandwidth = self._compute_bandwidth(X_particles_gpu[0, :])
    M_density_gpu = self._evolve_particles_gpu(X_particles_gpu, U_drift_gpu, bandwidth, ...)
    return self._transfer_results_to_cpu(X_particles_gpu, M_density_gpu)

def _transfer_inputs_to_gpu(self, ...): ...
def _initialize_particles_gpu(self, ...): ...
def _compute_bandwidth(self, ...): ...
def _evolve_particles_gpu(self, ...): ...
def _transfer_results_to_cpu(self, ...): ...
```

---

## Complexity Trade-offs Analysis

### What We Gained

**Performance**:
- ‚úÖ 1.83x speedup for large problems (N‚â•50k) on MPS
- ‚úÖ 3-5x expected on CUDA (estimated)
- ‚úÖ Automatic backend selection

**Architecture**:
- ‚úÖ Modular GPU utilities (separate files)
- ‚úÖ Backward compatible (CPU path unchanged)
- ‚úÖ Foundation for future optimization (JAX, custom kernels)

**Research Value**:
- ‚úÖ Documented GPU performance pitfalls
- ‚úÖ Educational value for similar problems
- ‚úÖ Strategy pattern foundation

### What We Paid

**Code Complexity**:
- ‚ö†Ô∏è 2.5x larger main solver file (150‚Üí393 lines)
- ‚ö†Ô∏è 680 additional lines in utilities
- ‚ö†Ô∏è 2 code paths to maintain (CPU + GPU)

**Maintenance Burden**:
- ‚ö†Ô∏è Need to test both pipelines
- ‚ö†Ô∏è Backend dependency (PyTorch optional)
- ‚ö†Ô∏è GPU-specific debugging complexity

**Development Time**:
- ‚ö†Ô∏è ~40 hours invested (Track B Phases 1, 2, 2.1)
- ‚ö†Ô∏è Ongoing maintenance required

---

## Final Verdict

### Is the Code Too Complicated?

**Answer**: ‚ö†Ô∏è **Moderately Complex, But Acceptable**

**Justification**:
1. **Clean separation**: CPU and GPU pipelines are independent
2. **Modular utilities**: GPU operations in separate files
3. **Minimal user impact**: Single parameter change (`backend="torch"`)
4. **Refactoring ready**: Strategy pattern exists but not integrated
5. **Production-ready**: Comprehensive tests, honest documentation

### Complexity Score: 6/10

| Aspect | Score | Reasoning |
|:-------|:------|:----------|
| **User API** | 9/10 | Minimal complexity (1 parameter) |
| **Conceptual Model** | 7/10 | Clear CPU/GPU separation |
| **Code Organization** | 6/10 | Good structure, some duplication |
| **Maintainability** | 5/10 | 2 paths to maintain, outdated comments |
| **Extensibility** | 7/10 | Strategy pattern ready, not integrated |
| **Documentation** | 8/10 | Comprehensive, needs minor updates |

**Overall**: **6.5/10** - Acceptable complexity for performance-critical code

### Comparison: Similar Libraries

**PyTorch** (eager execution):
- ~10,000 lines for core tensor operations
- Multiple dispatch for CPU/CUDA/MPS
- Verdict: Our 393 lines very modest in comparison

**JAX** (XLA compilation):
- ~5,000 lines for JIT compilation infrastructure
- Automatic fusion via XLA compiler
- Verdict: We could simplify with JAX (future work)

**SciPy** (scipy.stats.gaussian_kde):
- ~200 lines for KDE
- CPU-only implementation
- Verdict: Our GPU version (68 lines) is reasonable

---

## Recommendations Summary

### Immediate Actions (High Priority)

1. ‚úÖ **Fix outdated comments** (5 minutes)
   - Update GPU pipeline docstring with realistic speedup expectations

2. ‚ö†Ô∏è **Extract shared helper functions** (2-4 hours)
   - Reduce code duplication from ~40% to ~20%
   - Easier maintenance and testing

### Medium-Term Actions (Medium Priority)

3. üîÑ **Integrate StrategySelector** (6-8 hours)
   - Replace hard-coded if-else with strategy pattern
   - Enable automatic strategy selection
   - Foundation for JAX/custom kernels

4. üìù **Update documentation** (2 hours)
   - Add architecture decision record (ADR) for pipeline selection
   - Document code organization rationale
   - Add complexity justification

### Long-Term Actions (Optional)

5. üîß **Modularize GPU pipeline** (3-4 hours)
   - Split 143-line method into smaller focused methods
   - Easier to test and understand individual components

6. üöÄ **JAX implementation** (15-20 hours)
   - Use XLA fusion to simplify code
   - Expected 4-5x speedup on CUDA
   - Natural kernel fusion (fewer manual optimizations)

---

## Conclusion

The GPU acceleration integration has increased code complexity **moderately** (2.5x file size), but maintains **good architectural principles** through:

- ‚úÖ Clean separation of CPU and GPU pipelines
- ‚úÖ Modular utilities in separate files
- ‚úÖ Minimal user-facing complexity
- ‚úÖ Production-ready with comprehensive tests

**Primary concern**: ~40% code duplication and hard-coded dispatch

**Recommended path forward**:
1. **Quick wins**: Fix comments, extract helpers (4 hours total)
2. **Strategic refactor**: Integrate StrategySelector when time permits (6-8 hours)
3. **Future enhancement**: JAX implementation for cleaner code and better performance

**Overall assessment**: The complexity is **justified** for production GPU acceleration, but **incremental refactoring** would improve maintainability without major disruption.

---

**Document Date**: 2025-10-08
**Code Version**: Track B Phase 2.1 Complete (commit b18e9f5)
**Evaluation Scope**: `fp_particle.py`, `density_estimation.py`, `particle_utils.py`
