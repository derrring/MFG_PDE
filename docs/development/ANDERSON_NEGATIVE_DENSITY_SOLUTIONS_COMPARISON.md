# Anderson Acceleration Negative Density - Solutions Comparison

**Date**: 2025-10-05
**Status**: üîÑ **Analysis & Testing**
**Issue**: Anderson acceleration produces negative density values via overshoot

## Problem Summary

Anderson acceleration uses linear extrapolation of past iterates with potentially negative coefficients (from least-squares optimization), causing density M to overshoot below zero. This creates a dilemma:

- **Cannot clamp to zero**: Violates mass conservation `‚à´M dx = constant`
- **Cannot keep negative**: Violates probability interpretation `M(t,x) ‚â• 0`

## Three Proposed Solutions

### Option 1: Anderson for U Only, Standard Damping for M

**Concept**: Apply Anderson acceleration only to value function U, use standard Picard damping for density M.

#### Implementation
```python
# In FixedPointIterator.solve(), replace lines 288-307:

if self.use_anderson and self.anderson_accelerator is not None:
    # Apply Picard damping first
    U_damped = self.thetaUM * U_new_tmp_hjb + (1 - self.thetaUM) * U_old_current_picard_iter
    M_damped = self.thetaUM * M_new_tmp_fp + (1 - self.thetaUM) * M_old_current_picard_iter

    # Apply Anderson to U ONLY
    x_current_U = U_old_current_picard_iter.flatten()
    f_current_U = U_damped.flatten()
    x_next_U = self.anderson_accelerator.update(x_current_U, f_current_U, method="type1")

    self.U = x_next_U.reshape(U_old_current_picard_iter.shape)
    self.M = M_damped  # Standard damping only - preserves non-negativity
else:
    # Standard damping
    self.U = self.thetaUM * U_new_tmp_hjb + (1 - self.thetaUM) * U_old_current_picard_iter
    self.M = self.thetaUM * M_new_tmp_fp + (1 - self.thetaUM) * M_old_current_picard_iter
```

#### Pros
- ‚úÖ **Guaranteed non-negativity**: `M ‚â• 0` always (convex combination of non-negative values)
- ‚úÖ **Preserves mass conservation**: No clamping/projection needed
- ‚úÖ **Simple implementation**: Minimal code changes
- ‚úÖ **Still accelerates HJB**: U convergence is often the bottleneck
- ‚úÖ **Mathematically clean**: No ad-hoc fixes

#### Cons
- ‚ö†Ô∏è **Slower M convergence**: FP iteration not accelerated
- ‚ö†Ô∏è **Asymmetric treatment**: U and M handled differently
- ‚ö†Ô∏è **May need more iterations**: Overall convergence could slow down

#### Performance Prediction
- **Best case**: HJB is bottleneck ‚Üí Anderson on U gives 2-3√ó speedup
- **Worst case**: FP is bottleneck ‚Üí No acceleration, possible slowdown
- **Expected**: Moderate improvement (1.5-2√ó faster than no Anderson)

---

### Option 2: Non-Negativity Projection After Anderson

**Concept**: Apply Anderson to both U and M, then project M onto non-negative cone while preserving mass.

#### Implementation
```python
# In FixedPointIterator.solve(), after line 307:

if self.use_anderson and self.anderson_accelerator is not None:
    # [Existing Anderson code lines 288-307]
    ...
    self.U = x_next[:n_u].reshape(U_old_current_picard_iter.shape)
    self.M = x_next[n_u:].reshape(M_old_current_picard_iter.shape)

    # PROJECT M to non-negative cone while preserving mass
    for t_idx in range(self.M.shape[0]):
        M_t = self.M[t_idx, :]

        if (M_t < 0).any():
            # Projection: M_proj = argmin ||M - M_anderson||^2
            #             subject to: M ‚â• 0, ‚à´M dx = ‚à´M_anderson dx

            total_mass = np.trapezoid(M_t, dx=Dx)

            # Simple projection: clamp negatives, redistribute to maintain mass
            M_pos = np.maximum(M_t, 0)
            mass_pos = np.trapezoid(M_pos, dx=Dx)

            if mass_pos > 1e-10:
                # Rescale to preserve total mass
                self.M[t_idx, :] = M_pos * (total_mass / mass_pos)
            else:
                # Fallback: uniform distribution
                self.M[t_idx, :] = total_mass / Lx  # Uniform
```

#### Pros
- ‚úÖ **Full Anderson acceleration**: Both U and M accelerated
- ‚úÖ **Enforces non-negativity**: M ‚â• 0 guaranteed
- ‚úÖ **Preserves mass**: `‚à´M dx` maintained (approximately)
- ‚úÖ **Fastest convergence**: Maximum acceleration potential

#### Cons
- ‚ùå **Complex implementation**: Non-trivial projection algorithm
- ‚ùå **Approximation errors**: Projection introduces additional error
- ‚ùå **Not exact mass conservation**: Numerical integration errors accumulate
- ‚ùå **Potential instability**: Projection + extrapolation can oscillate
- ‚ö†Ô∏è **Computationally expensive**: Projection at every iteration

#### Performance Prediction
- **Best case**: Both HJB & FP accelerated ‚Üí 3-5√ó speedup
- **Worst case**: Projection causes instability ‚Üí Divergence or slowdown
- **Expected**: 2-3√ó faster but with occasional instability

#### Mathematical Concerns

**Problem**: This is not truly conservative!

```
‚à´M_projected dx ‚âà ‚à´M_anderson dx  (numerical integration error)
```

Over many iterations, errors accumulate. The rescaling is a **heuristic**, not a rigorous projection.

**Rigorous projection** would require:
```
min ||M - M_anderson||^2
s.t. M ‚â• 0
     ‚à´M dx = const  (continuous constraint!)
```

This is a constrained QP problem that needs discretization-aware formulation.

---

### Option 3: Reduce Anderson Aggressiveness

**Concept**: Keep Anderson for both U and M, but reduce aggressiveness to avoid negative overshoot.

#### Implementation A: Reduce Anderson Depth
```python
# In test/example configuration:
mfg_solver = FixedPointIterator(
    problem,
    hjb_solver=hjb_solver,
    fp_solver=fp_solver,
    use_anderson=True,
    anderson_depth=2,  # Reduced from 5 (fewer past iterates)
    anderson_beta=0.5, # Reduced mixing (if parameter exists)
    thetaUM=0.5,
)
```

#### Implementation B: Safeguard Anderson Update
```python
# In FixedPointIterator.solve(), after Anderson update:

if self.use_anderson and self.anderson_accelerator is not None:
    # [Existing Anderson code]
    ...
    M_anderson = x_next[n_u:].reshape(M_old_current_picard_iter.shape)

    # SAFEGUARD: If Anderson produces negatives, fall back to damping
    if (M_anderson < 0).any():
        print(f"  Anderson overshoot detected, using standard damping for M")
        self.M = self.thetaUM * M_new_tmp_fp + (1 - self.thetaUM) * M_old_current_picard_iter
    else:
        self.M = M_anderson

    self.U = x_next[:n_u].reshape(U_old_current_picard_iter.shape)
```

#### Pros
- ‚úÖ **Simple safeguard**: Fallback to damping when needed
- ‚úÖ **Preserves mass**: No projection, uses standard damping for negative cases
- ‚úÖ **Adaptive**: Anderson when safe, damping when overshoot
- ‚úÖ **Easy to tune**: Single parameter (depth or beta)

#### Cons
- ‚ö†Ô∏è **Unpredictable**: Sometimes accelerates, sometimes doesn't
- ‚ö†Ô∏è **Parameter tuning**: Need to find sweet spot (problem-dependent)
- ‚ö†Ô∏è **Suboptimal acceleration**: Reduced depth means less speedup
- ‚ùå **Doesn't solve root cause**: Just avoids symptoms

#### Performance Prediction
- **Best case**: Reduced depth sufficient ‚Üí 1.5-2√ó speedup, no negatives
- **Worst case**: Still produces negatives ‚Üí Frequent fallbacks, 1.2√ó speedup
- **Expected**: Moderate acceleration with occasional fallbacks

---

## Comparative Analysis

### Mass Conservation Quality

| Option | Mass Conservation | Notes |
|:-------|:-----------------|:------|
| **Option 1** | ‚úÖ **Exact** | Convex combination preserves `‚à´M dx` exactly |
| **Option 2** | ‚ö†Ô∏è **Approximate** | Projection error accumulates over iterations |
| **Option 3** | ‚úÖ **Exact** (fallback) | Safeguard uses standard damping |

**Winner**: Option 1 (or Option 3 with fallback)

### Non-Negativity Guarantee

| Option | Guarantee | Notes |
|:-------|:----------|:------|
| **Option 1** | ‚úÖ **Always** | `M = Œ∏*M_new + (1-Œ∏)*M_old`, both ‚â• 0 |
| **Option 2** | ‚úÖ **After projection** | Projection enforces M ‚â• 0 |
| **Option 3** | ‚úÖ **With fallback** | Falls back to damping if negative |

**Winner**: All options guarantee non-negativity

### Convergence Speed

| Option | Expected Speedup | Stability |
|:-------|:----------------|:----------|
| **Option 1** | 1.5-2√ó | ‚úÖ High |
| **Option 2** | 2-3√ó | ‚ö†Ô∏è Medium (projection can cause oscillation) |
| **Option 3** | 1.2-2√ó | ‚úÖ High (adaptive fallback) |

**Winner**: Option 2 (if stable), Option 1 (for reliability)

### Implementation Complexity

| Option | Code Changes | Maintenance |
|:-------|:------------|:------------|
| **Option 1** | ‚úÖ Minimal (~10 lines) | ‚úÖ Simple |
| **Option 2** | ‚ùå Significant (~30 lines + projection) | ‚ùå Complex |
| **Option 3** | ‚úÖ Small (~15 lines) | ‚úÖ Simple |

**Winner**: Option 1 or Option 3

### Theoretical Soundness

| Option | Mathematical Rigor | Notes |
|:-------|:------------------|:------|
| **Option 1** | ‚úÖ **Clean** | Standard convex combination |
| **Option 2** | ‚ö†Ô∏è **Heuristic** | Projection is approximate |
| **Option 3** | ‚úÖ **Principled** | Adaptive with fallback |

**Winner**: Option 1

---

## Recommendation Matrix

### By Priority

| Priority | Best Option | Reason |
|:---------|:-----------|:-------|
| **Mass conservation** | Option 1 | Exact preservation |
| **Convergence speed** | Option 2 | Full acceleration (if stable) |
| **Robustness** | Option 1 | No approximations |
| **Simplicity** | Option 1 | Minimal code changes |
| **Adaptability** | Option 3 | Handles various problems |

### By Problem Type

| Problem Characteristics | Recommended | Reason |
|:----------------------|:-----------|:-------|
| **HJB is bottleneck** | **Option 1** | Anderson on U sufficient |
| **FP is bottleneck** | Option 2 or 3 | Need M acceleration |
| **High stochasticity (large œÉ)** | **Option 1** | More stable |
| **Tight coupling** | Option 2 | Need both accelerated |
| **Research/exploration** | Option 3 | Easy to tune |

---

## Testing Plan

### Test 1: Convergence Rate
Compare iterations to convergence for each option:
- Problem: Standard 1D MFG (Nx=50, Nt=20, œÉ=0.1)
- Tolerance: 1e-4
- Metric: Iterations to convergence

### Test 2: Mass Conservation Error
Track `|‚à´M(t,x)dx - ‚à´M(0,x)dx|` over iterations:
- Metric: Maximum mass error across all timesteps
- Option 1 should have machine precision (~1e-15)
- Option 2 may accumulate error (~1e-8 to 1e-12)

### Test 3: Negative Density Frequency
Count how often M < 0 occurs:
- Option 1: 0 occurrences (guaranteed)
- Option 2: 0 after projection
- Option 3: Depends on depth parameter

### Test 4: Computational Cost
Wall-clock time per iteration:
- Option 1: Baseline (Anderson on U only)
- Option 2: +overhead from projection
- Option 3: Baseline (no extra computation)

---

## Final Recommendation: **Option 1** ‚úÖ

**Rationale**:
1. **Mathematically clean**: No approximations, exact conservation
2. **Simple implementation**: Minimal code changes
3. **Robust**: Guaranteed non-negativity without projection
4. **Sufficient acceleration**: HJB is typically the bottleneck
5. **Easy to understand**: Clear physical/mathematical interpretation

**When to consider alternatives**:
- **Option 2**: If benchmarking shows FP is bottleneck AND projection remains stable
- **Option 3**: If Option 1 convergence is too slow for specific problem class

---

## Implementation Order

1. ‚úÖ **Implement Option 1** (primary solution)
2. ‚ö†Ô∏è **Benchmark against no Anderson** (establish baseline)
3. üî¨ **Optionally implement Option 3** (for comparison)
4. üìä **Compare convergence rates and robustness**
5. üìù **Document findings and update defaults**

---

**Next**: Proceed with Option 1 implementation
