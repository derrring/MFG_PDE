# Base configuration for neural methods
paradigm: neural

# Network architecture
architecture:
  hidden_dims: [64, 64, 64]
  activation: "tanh"
  initialization: "xavier_normal"
  final_activation: null

# Training parameters
training:
  optimizer: "adam"
  learning_rate: 1e-3
  batch_size: 1024
  epochs: 10000
  lr_scheduler: "cosine"

# Loss function weights
loss_weights:
  physics: 1.0
  boundary: 10.0
  initial: 10.0
  data: 1.0

# Sampling strategy
sampling:
  n_interior: 10000
  n_boundary: 1000
  n_initial: 1000
  strategy: "uniform"

# Training monitoring
monitoring:
  log_interval: 100
  checkpoint_interval: 1000
  validation_interval: 500
  early_stopping_patience: 2000

# Hardware settings
device: "auto"  # auto, cpu, cuda, mps
precision: "float32"
