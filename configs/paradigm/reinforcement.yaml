# Base configuration for reinforcement learning methods
paradigm: reinforcement

# Population settings
population:
  size: 1000  # finite population size, or null for mean field
  agent_type: "continuous"
  state_dim: 2
  action_dim: 2

# Environment settings
environment:
  time_horizon: 1.0
  dt: 0.01
  reward_type: "mfg"
  observation_type: "state_density"

# Training parameters
training:
  algorithm: "mfrl"  # mfrl, nash_q, maddpg, ppo
  total_timesteps: 1000000
  learning_rate: 3e-4
  batch_size: 256
  buffer_size: 100000

# Network architecture (for value/policy networks)
networks:
  policy:
    hidden_dims: [128, 128]
    activation: "relu"
  value:
    hidden_dims: [128, 128]
    activation: "relu"

# Algorithm-specific parameters
algorithm_params:
  gamma: 0.99
  tau: 0.005
  exploration_noise: 0.1
  policy_noise: 0.2

# Evaluation
evaluation:
  eval_freq: 10000
  n_eval_episodes: 10
  nash_gap_tolerance: 1e-3

# Logging
logging:
  use_tensorboard: true
  use_wandb: false
  log_interval: 1000
