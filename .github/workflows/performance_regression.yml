name: Performance Regression Tests

on:
  pull_request:
    branches: [main]
    paths:
      - 'mfg_pde/**/*.py'
      - 'benchmarks/**/*.py'
      - '.github/workflows/performance_regression.yml'

permissions:
  contents: read
  pull-requests: write  # Required to post comments on PRs

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v5
        with:
          fetch-depth: 0  # Need full history for baseline comparison

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'  # Package requires Python >=3.12
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install pytest-benchmark

      - name: Run benchmarks on PR branch
        run: |
          pytest benchmarks/test_solver_performance.py \
            --benchmark-only \
            --benchmark-min-rounds=3 \
            --benchmark-warmup=on \
            --benchmark-json=.benchmarks/pr_results.json \
            -v
        continue-on-error: true

      - name: Checkout main branch
        run: |
          git fetch origin main:main
          git checkout main

      - name: Run benchmarks on main branch
        run: |
          pytest benchmarks/test_solver_performance.py \
            --benchmark-only \
            --benchmark-min-rounds=3 \
            --benchmark-warmup=on \
            --benchmark-json=.benchmarks/main_results.json \
            -v
        continue-on-error: true

      - name: Checkout PR branch again
        run: |
          git checkout ${{ github.event.pull_request.head.sha }}

      - name: Compare benchmark results
        id: compare
        run: |
          python scripts/compare_benchmarks.py \
            --pr-results .benchmarks/pr_results.json \
            --main-results .benchmarks/main_results.json \
            --threshold 15 \
            --output .benchmarks/comparison.md
        continue-on-error: true

      - name: Comment benchmark results on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let comment = '## ðŸ“Š Performance Benchmark Results\n\n';

            try {
              const comparison = fs.readFileSync('.benchmarks/comparison.md', 'utf8');
              comment += comparison;
            } catch (error) {
              comment += 'âš ï¸ Could not generate benchmark comparison. Check workflow logs for details.\n';
              comment += `\nError: ${error.message}`;
            }

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Benchmark Results')
            );

            // Update or create comment
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: benchmark-results
          path: |
            .benchmarks/*.json
            .benchmarks/*.md
          retention-days: 30

  # New PerformanceTracker-based benchmarking
  performance-tracker:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install psutil  # For memory profiling

      - name: Run standard benchmark suite
        run: |
          python benchmarks/run_benchmarks.py \
            --category small \
            --profile-memory \
            --check-regression \
            --history-dir benchmarks/history

      - name: Upload performance history
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: performance-history
          path: benchmarks/history/*.json
          retention-days: 90

      - name: Generate performance report
        if: always()
        run: |
          mkdir -p .benchmarks
          python -c "
          from benchmarks.performance_tracker import PerformanceTracker
          from benchmarks.standard_problems import get_problems_by_category
          import json

          tracker = PerformanceTracker('benchmarks/history')
          report = {'problems': []}

          for problem in get_problems_by_category('small'):
              stats = tracker.get_statistics(problem.solver_type, problem.name)
              if stats.get('count', 0) > 0:
                  report['problems'].append({
                      'name': problem.name,
                      'solver': problem.solver_type,
                      'stats': stats
                  })

          with open('.benchmarks/tracker_report.json', 'w') as f:
              json.dump(report, f, indent=2)
          "

      - name: Upload tracker report
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: tracker-report
          path: .benchmarks/tracker_report.json
          retention-days: 30
