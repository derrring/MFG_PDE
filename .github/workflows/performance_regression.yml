name: Performance Regression Tests

on:
  pull_request:
    branches: [main]
    paths:
      - 'mfg_pde/**/*.py'
      - 'benchmarks/**/*.py'
      - '.github/workflows/performance_regression.yml'

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for baseline comparison

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'  # Package requires Python >=3.12
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install pytest-benchmark

      - name: Run benchmarks on PR branch
        run: |
          pytest benchmarks/test_solver_performance.py \
            --benchmark-only \
            --benchmark-min-rounds=3 \
            --benchmark-warmup=on \
            --benchmark-json=.benchmarks/pr_results.json \
            -v
        continue-on-error: true

      - name: Checkout main branch
        run: |
          git fetch origin main:main
          git checkout main

      - name: Run benchmarks on main branch
        run: |
          pytest benchmarks/test_solver_performance.py \
            --benchmark-only \
            --benchmark-min-rounds=3 \
            --benchmark-warmup=on \
            --benchmark-json=.benchmarks/main_results.json \
            -v
        continue-on-error: true

      - name: Checkout PR branch again
        run: |
          git checkout ${{ github.event.pull_request.head.sha }}

      - name: Compare benchmark results
        id: compare
        run: |
          python scripts/compare_benchmarks.py \
            --pr-results .benchmarks/pr_results.json \
            --main-results .benchmarks/main_results.json \
            --threshold 15 \
            --output .benchmarks/comparison.md
        continue-on-error: true

      - name: Comment benchmark results on PR
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let comment = '## 📊 Performance Benchmark Results\n\n';

            try {
              const comparison = fs.readFileSync('.benchmarks/comparison.md', 'utf8');
              comment += comparison;
            } catch (error) {
              comment += '⚠️ Could not generate benchmark comparison. Check workflow logs for details.\n';
              comment += `\nError: ${error.message}`;
            }

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Benchmark Results')
            );

            // Update or create comment
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            .benchmarks/*.json
            .benchmarks/*.md
          retention-days: 30
