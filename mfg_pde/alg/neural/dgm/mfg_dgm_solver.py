"""
Mean Field Game solver using Deep Galerkin Methods.

This module implements a complete DGM solver for Mean Field Games,
enabling efficient solution of high-dimensional MFG systems using
neural network function approximation and Monte Carlo sampling.

Mathematical Framework:
- HJB Equation: ∂u/∂t + H(x, ∇u, m) = 0
- FP Equation: ∂m/∂t - div(m ∇H_p(x, ∇u, m)) = σ²/2 Δm
- Coupling: m and u are mutually dependent through Hamiltonian

DGM Approach:
- Neural Approximation: u(t,x) ≈ U_θ(t,x), m(t,x) ≈ M_φ(t,x)
- Monte Carlo Loss: L = E[|HJB_residual|²] + E[|FP_residual|²] + BC + IC
- High-Dimensional: Efficient for d > 5 where grid methods fail
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any

import numpy as np

from mfg_pde.utils.mfg_logging import get_logger

from .base_dgm import BaseDGMSolver, DGMConfig
from .sampling import MonteCarloSampler, QuasiMonteCarloSampler

if TYPE_CHECKING:
    from numpy.typing import NDArray

    from mfg_pde.core.mfg_problem import MFGProblem

# Import with availability checking
try:
    import torch
    import torch.nn as nn  # noqa: F401
    import torch.optim as optim

    from .architectures import DeepGalerkinNetwork, HighDimMLP

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = get_logger(__name__)
if TORCH_AVAILABLE:

    class MFGDGMSolver(BaseDGMSolver):
        """
        Deep Galerkin Method solver for Mean Field Games.

        This solver uses neural network function approximation to solve
        high-dimensional MFG systems that are computationally intractable
        with traditional grid-based methods.

        Key Features:
        - Handles dimensions d > 5 efficiently
        - Simultaneous approximation of u(t,x) and m(t,x)
        - Monte Carlo sampling for high-dimensional integration
        - Variance reduction for computational efficiency
        - Adaptive sampling based on residual analysis

        Mathematical Formulation:
        - Value Function: u(t,x) approximated by neural network U_θ(t,x)
        - Density Function: m(t,x) approximated by neural network M_φ(t,x)
        - Loss Function: L = L_HJB + L_FP + L_BC + L_IC + L_coupling
        """

        def __init__(
            self,
            problem: MFGProblem,
            config: DGMConfig | None = None,
            **kwargs: Any,
        ):
            """
            Initialize MFG DGM solver.

            Args:
                problem: High-dimensional MFG problem
                config: DGM configuration
                **kwargs: Additional solver arguments
            """
            super().__init__(problem, config, **kwargs)

            # Setup device
            self.device = torch.device(self._get_device())
            self.logger.info(f"Using device: {self.device}")

            # Log training setup
            self._log_training_setup()

        def _setup_neural_networks(self) -> None:
            """Setup neural networks for value and density approximation."""
            input_dim = self.dimension + 1  # Space + time

            # Value function network U_θ(t,x)
            self.value_network = DeepGalerkinNetwork(
                input_dim=input_dim,
                hidden_layers=self.config.hidden_layers,
                output_dim=1,
                activation=self.config.activation,
                use_batch_norm=self.config.batch_normalization,
            ).to(self.device)

            # Density function network M_φ(t,x)
            self.density_network = HighDimMLP(
                input_dim=input_dim,
                output_dim=1,
                base_width=self.config.hidden_layers[0] if self.config.hidden_layers else 256,
                num_layers=len(self.config.hidden_layers) if self.config.hidden_layers else 4,
                activation=self.config.activation,
            ).to(self.device)

            self.logger.info(
                f"Networks initialized: {sum(p.numel() for p in self.value_network.parameters())} + "
                f"{sum(p.numel() for p in self.density_network.parameters())} parameters"
            )

        def _setup_sampling(self) -> None:
            """Setup high-dimensional sampling strategies."""
            # Infer domain bounds from problem, use getattr for optional attribute
            domain = getattr(self.problem, "domain", None)
            if domain is not None:
                if isinstance(domain, list | tuple) and len(domain) == 2:
                    # 1D domain (min, max)
                    self.domain_bounds = [domain]
                else:
                    # Multi-dimensional domain - assume alternating min/max
                    domain_list = list(domain)
                    self.domain_bounds = [(domain_list[i], domain_list[i + 1]) for i in range(0, len(domain_list), 2)]
            else:
                # Default hypercube domain
                self.domain_bounds = [(0.0, 1.0)] * self.dimension

            # Initialize sampler
            if self.config.sampling_strategy == "quasi_monte_carlo":
                self.sampler = QuasiMonteCarloSampler(self.domain_bounds, self.dimension)
            else:
                self.sampler = MonteCarloSampler(self.domain_bounds, self.dimension)

            self.logger.info(f"Sampling setup: {self.config.sampling_strategy} in {self.dimension}D")

        def _setup_variance_reduction(self) -> None:
            """Setup variance reduction techniques."""
            # Control variates setup (simplified)
            self.use_control_variates = self.config.use_control_variates

            # Baseline function for variance reduction
            if self.config.baseline_function:
                self.logger.info(f"Variance reduction with baseline: {self.config.baseline_function}")

        def _initialize_training(self) -> None:
            """Initialize PyTorch training infrastructure."""
            # Combine parameters from both networks
            params = list(self.value_network.parameters()) + list(self.density_network.parameters())

            # Optimizer
            if self.config.optimizer == "adam":
                self.optimizer = optim.Adam(params, lr=self.config.learning_rate)
            elif self.config.optimizer == "lbfgs":
                self.optimizer = optim.LBFGS(params, lr=self.config.learning_rate)
            else:
                self.optimizer = optim.SGD(params, lr=self.config.learning_rate)

            # Learning rate scheduler
            if self.config.scheduler == "cosine":
                self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config.max_epochs)
            elif self.config.scheduler == "exponential":
                self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.99)
            else:
                self.scheduler = None

            self.logger.info(f"Training initialized: {self.config.optimizer} optimizer")

        def _sample_interior_points(self, num_points: int) -> NDArray:
            """Sample interior points for physics loss computation."""
            T_final = getattr(self.problem, "T", 1.0)
            time_bounds = (0.0, T_final)
            points = self.sampler.sample_interior(num_points, time_bounds)
            return points

        def _sample_boundary_points(self, num_points: int) -> NDArray:
            """Sample boundary points for boundary condition loss."""
            T_final = getattr(self.problem, "T", 1.0)
            time_bounds = (0.0, T_final)
            points = self.sampler.sample_boundary(num_points, time_bounds)
            return points

        def _sample_initial_points(self, num_points: int) -> NDArray:
            """Sample initial points for initial condition loss."""
            points = self.sampler.sample_initial(num_points)
            return points

        def _compute_physics_residual(self, points: NDArray) -> NDArray:
            """Compute HJB and FP equation residuals."""
            # Convert to torch tensors
            points_torch = torch.tensor(points, dtype=torch.float32, device=self.device, requires_grad=True)

            # Split time and space coordinates
            t = points_torch[:, 0:1]
            x = points_torch[:, 1:]

            # Evaluate networks
            u = self.value_network(points_torch)
            m = torch.exp(self.density_network(points_torch))  # Ensure positivity

            # Compute gradients using autograd
            u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]
            u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]

            m_t = torch.autograd.grad(m.sum(), t, create_graph=True)[0]
            torch.autograd.grad(m.sum(), x, create_graph=True)[0]

            # Simplified Hamiltonian (quadratic cost)
            H = 0.5 * torch.sum(u_x**2, dim=1, keepdim=True)
            H_p = u_x  # ∇_p H = p for quadratic H

            # HJB residual: ∂u/∂t + H(x, ∇u, m) = 0
            hjb_residual = u_t + H

            # FP residual: ∂m/∂t - div(m ∇H_p) = 0 (simplified, no diffusion)
            div_term = torch.autograd.grad((m * H_p).sum(), x, create_graph=True)[0]
            fp_residual = m_t - torch.sum(div_term, dim=1, keepdim=True)

            # Combine residuals
            residuals = torch.cat([hjb_residual.flatten(), fp_residual.flatten()])

            return residuals.detach().cpu().numpy()

        def _compute_boundary_residual(self, points: NDArray) -> NDArray:
            """Compute boundary condition residuals."""
            # Simplified Dirichlet boundary conditions
            points_torch = torch.tensor(points, dtype=torch.float32, device=self.device)
            u_boundary = self.value_network(points_torch)

            # Zero Dirichlet boundary condition (can be generalized)
            boundary_residual = u_boundary.flatten()

            return boundary_residual.detach().cpu().numpy()

        def _compute_initial_residual(self, points: NDArray) -> NDArray:
            """Compute initial condition residuals."""
            # Add t=0 to spatial points
            t_zero = np.zeros((len(points), 1))
            initial_points = np.hstack([t_zero, points])

            points_torch = torch.tensor(initial_points, dtype=torch.float32, device=self.device)

            # Evaluate networks at t=0
            u_initial = self.value_network(points_torch)
            m_initial = torch.exp(self.density_network(points_torch))

            # Initial condition residuals (problem-specific)
            u_ic_residual = u_initial.flatten()  # Simplified: u(0,x) = 0
            m_ic_residual = m_initial.flatten() - 1.0  # Simplified: m(0,x) = 1

            residuals = torch.cat([u_ic_residual, m_ic_residual])
            return residuals.detach().cpu().numpy()

        def _compute_physics_loss(self, points: NDArray) -> float:
            """Compute physics loss from residuals."""
            residuals = self._compute_physics_residual(points)
            return float(np.mean(residuals**2))

        def _compute_boundary_loss(self, points: NDArray) -> float:
            """Compute boundary loss from residuals."""
            residuals = self._compute_boundary_residual(points)
            return float(np.mean(residuals**2))

        def _compute_initial_loss(self, points: NDArray) -> float:
            """Compute initial loss from residuals."""
            residuals = self._compute_initial_residual(points)
            return float(np.mean(residuals**2))

        def _optimization_step(self, loss: float) -> None:
            """Perform optimization step."""
            # Convert loss back to torch tensor for optimization
            # Note: In practice, this should be done within PyTorch computational graph
            # This is a simplified interface - actual implementation would maintain
            # the computational graph throughout the loss computation

            # Zero gradients
            self.optimizer.zero_grad()

            # Recompute loss within computational graph for gradient computation
            interior_points = self._sample_interior_points(self.config.batch_size)
            boundary_points = self._sample_boundary_points(self.config.batch_size // 4)
            initial_points = self._sample_initial_points(self.config.batch_size // 4)

            # Compute losses within graph
            physics_loss_torch = self._compute_physics_loss_torch(interior_points)
            boundary_loss_torch = self._compute_boundary_loss_torch(boundary_points)
            initial_loss_torch = self._compute_initial_loss_torch(initial_points)

            total_loss_torch = (
                self.config.physics_weight * physics_loss_torch
                + self.config.boundary_weight * boundary_loss_torch
                + self.config.initial_weight * initial_loss_torch
            )

            # Backward pass
            total_loss_torch.backward()
            self.optimizer.step()

            # Update learning rate
            if self.scheduler:
                self.scheduler.step()

        def _compute_physics_loss_torch(self, points: NDArray) -> torch.Tensor:
            """Compute physics loss maintaining PyTorch computational graph."""
            points_torch = torch.tensor(points, dtype=torch.float32, device=self.device, requires_grad=True)

            t = points_torch[:, 0:1]
            x = points_torch[:, 1:]

            u = self.value_network(points_torch)
            m = torch.exp(self.density_network(points_torch))

            # Compute gradients
            u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]
            u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]

            m_t = torch.autograd.grad(m.sum(), t, create_graph=True)[0]

            # Simplified Hamiltonian
            H = 0.5 * torch.sum(u_x**2, dim=1, keepdim=True)
            H_p = u_x

            # PDE residuals
            hjb_residual = u_t + H
            fp_residual = m_t - torch.autograd.grad((m * H_p).sum(), x, create_graph=True)[0].sum(dim=1, keepdim=True)

            # Combined physics loss
            physics_loss = torch.mean(hjb_residual**2) + torch.mean(fp_residual**2)

            return physics_loss

        def _compute_boundary_loss_torch(self, points: NDArray) -> torch.Tensor:
            """Compute boundary loss maintaining computational graph."""
            points_torch = torch.tensor(points, dtype=torch.float32, device=self.device)
            u_boundary = self.value_network(points_torch)

            # Zero Dirichlet boundary condition
            boundary_loss = torch.mean(u_boundary**2)

            return boundary_loss

        def _compute_initial_loss_torch(self, points: NDArray) -> torch.Tensor:
            """Compute initial condition loss maintaining computational graph."""
            t_zero = np.zeros((len(points), 1))
            initial_points = np.hstack([t_zero, points])

            points_torch = torch.tensor(initial_points, dtype=torch.float32, device=self.device)

            u_initial = self.value_network(points_torch)
            m_initial = torch.exp(self.density_network(points_torch))

            # Initial condition losses
            u_ic_loss = torch.mean(u_initial**2)  # u(0,x) = 0
            m_ic_loss = torch.mean((m_initial - 1.0) ** 2)  # m(0,x) = 1

            return u_ic_loss + m_ic_loss

        def _adaptive_sampling_update(self) -> None:
            """Update sampling based on current residual analysis."""
            if not self.config.adaptive_sampling:
                return

            # Sample test points to evaluate residuals
            test_points = self._sample_interior_points(1000)
            residuals = self._compute_physics_residual(test_points)

            # Find high-residual regions
            high_residual_mask = np.abs(residuals) > self.config.residual_threshold

            if np.any(high_residual_mask):
                num_high_residual = np.sum(high_residual_mask)
                self.logger.info(f"Adaptive sampling: {num_high_residual} high-residual points detected")

                # Add more points in high-residual regions (implementation can be enhanced)
                additional_points = min(self.config.max_adaptive_points, num_high_residual * 2)
                self.config.num_interior_points += additional_points

        def _extract_value_network(self) -> Any:
            """Extract trained value function network."""
            return self.value_network

        def _extract_density_network(self) -> Any:
            """Extract trained density function network."""
            return self.density_network

        def _evaluate_on_grid(self) -> tuple[NDArray, NDArray]:
            """Evaluate trained networks on regular grid for analysis."""
            # Create evaluation grid (simplified for 1D, can be extended)
            if self.dimension == 1:
                x_eval = np.linspace(self.domain_bounds[0][0], self.domain_bounds[0][1], 100)
                T_final = getattr(self.problem, "T", 1.0)
                t_eval = np.linspace(0, T_final, 50)

                X_eval, T_eval = np.meshgrid(x_eval, t_eval)
                eval_points = np.column_stack([T_eval.ravel(), X_eval.ravel()])

            else:
                # Multi-dimensional evaluation (simplified)
                num_points_per_dim = int(100 ** (1.0 / self.dimension))  # Maintain reasonable grid size
                eval_grids = [np.linspace(bounds[0], bounds[1], num_points_per_dim) for bounds in self.domain_bounds]

                # Create multi-dimensional grid
                grid_arrays = np.meshgrid(*eval_grids, indexing="ij")
                spatial_points = np.column_stack([grid.ravel() for grid in grid_arrays])

                # Add time dimension
                T_final = getattr(self.problem, "T", 1.0)
                t_eval = np.linspace(0, T_final, 20)
                eval_points = []

                for t in t_eval:
                    time_points = np.column_stack([np.full(len(spatial_points), t), spatial_points])
                    eval_points.append(time_points)

                eval_points = np.vstack(eval_points)

            # Evaluate networks
            with torch.no_grad():
                eval_torch = torch.tensor(eval_points, dtype=torch.float32, device=self.device)

                u_eval = self.value_network(eval_torch).cpu().numpy()
                m_eval = torch.exp(self.density_network(eval_torch)).cpu().numpy()

            self.logger.info(f"Network evaluation completed on {len(eval_points)} grid points")

            return u_eval.reshape(-1), m_eval.reshape(-1)

else:
    # Placeholder when PyTorch not available
    class MFGDGMSolver:  # type: ignore[no-redef]
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch is required for MFG DGM solver. Install with: pip install torch")
