#!/usr/bin/env python3
"""
Log Analysis Utilities for MFG_PDE
=================================

Tools for analyzing and summarizing log files generated by the MFG_PDE
logging system, providing insights into solver performance, errors, and
research patterns.
"""

from __future__ import annotations

import json
import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import numpy as np

from .logger import get_logger


class LogAnalyzer:
    """Comprehensive log analysis for MFG_PDE research and debugging."""

    def __init__(self, log_file_path: str):
        """
        Initialize log analyzer.

        Args:
            log_file_path: Path to log file to analyze
        """
        self.log_file_path = Path(log_file_path)
        self.logger = get_logger(__name__)

        if not self.log_file_path.exists():
            raise FileNotFoundError(f"Log file not found: {log_file_path}")

        self.entries: list[dict[str, Any]] = []
        self.parsed = False

    def parse_log_file(self):
        """Parse the log file into structured entries."""
        if self.parsed:
            return

        self.logger.info(f"Parsing log file: {self.log_file_path}")

        # Pattern to match MFG_PDE log format
        log_pattern = re.compile(
            r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) - "  # timestamp
            r"([^-]+?) - "  # logger name
            r"(\w+)\s+ - "  # level
            r"(.*?)(?:\s+\[([^\]]+)\])?$"  # message and optional location
        )

        with open(self.log_file_path) as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                match = log_pattern.match(line)
                if match:
                    timestamp_str, logger_name, level, message, location = match.groups()

                    try:
                        timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                    except ValueError:
                        continue

                    entry = {
                        "line_number": line_num,
                        "timestamp": timestamp,
                        "logger": logger_name.strip(),
                        "level": level,
                        "message": message,
                        "location": location,
                        "raw_line": line,
                    }

                    self.entries.append(entry)

        self.parsed = True
        self.logger.info(f"Parsed {len(self.entries)} log entries")

    def get_summary_statistics(self) -> dict[str, Any]:
        """Get overall summary statistics from the log."""
        if not self.parsed:
            self.parse_log_file()

        if not self.entries:
            return {}

        # Basic statistics
        total_entries = len(self.entries)
        start_time = self.entries[0]["timestamp"]
        end_time = self.entries[-1]["timestamp"]
        duration = end_time - start_time

        # Count by level
        level_counts = Counter(entry["level"] for entry in self.entries)

        # Count by logger
        logger_counts = Counter(entry["logger"] for entry in self.entries)

        # Error and warning analysis
        errors = [e for e in self.entries if e["level"] in ("ERROR", "CRITICAL")]
        warnings = [e for e in self.entries if e["level"] == "WARNING"]

        return {
            "total_entries": total_entries,
            "start_time": start_time,
            "end_time": end_time,
            "duration": duration,
            "level_counts": dict(level_counts),
            "logger_counts": dict(logger_counts),
            "error_count": len(errors),
            "warning_count": len(warnings),
            "recent_errors": [e["message"] for e in errors[-5:]],  # Last 5 errors
            "recent_warnings": [e["message"] for e in warnings[-5:]],  # Last 5 warnings
        }

    def analyze_solver_performance(self) -> dict[str, Any]:
        """Analyze solver performance from log entries."""
        if not self.parsed:
            self.parse_log_file()

        performance_data: dict[str, list[Any]] = {
            "solver_sessions": [],
            "convergence_data": [],
            "timing_data": [],
            "mass_conservation": [],
        }

        # Find solver sessions
        solver_start_pattern = re.compile(r"Initializing (\w+)")
        solver_complete_pattern = re.compile(r"(\w+) completed - Status: (\w+)")
        performance_pattern = re.compile(r"Performance - ([^:]+): ([\d.]+)s")
        convergence_pattern = re.compile(r"Iteration (\d+)/(\d+).*Error: ([\d.e-]+)")

        current_solver = None
        for entry in self.entries:
            message = entry["message"]

            # Solver start
            match = solver_start_pattern.search(message)
            if match:
                current_solver = {
                    "solver_name": match.group(1),
                    "start_time": entry["timestamp"],
                    "iterations": [],
                    "errors": [],
                }
                continue

            # Solver completion
            match = solver_complete_pattern.search(message)
            if match and current_solver:
                current_solver["end_time"] = entry["timestamp"]
                current_solver["status"] = match.group(2)
                current_solver["duration"] = (entry["timestamp"] - current_solver["start_time"]).total_seconds()
                performance_data["solver_sessions"].append(current_solver)
                current_solver = None
                continue

            # Performance data
            match = performance_pattern.search(message)
            if match:
                performance_data["timing_data"].append(
                    {
                        "operation": match.group(1),
                        "duration": float(match.group(2)),
                        "timestamp": entry["timestamp"],
                    }
                )
                continue

            # Convergence data
            match = convergence_pattern.search(message)
            if match and current_solver:
                iteration = int(match.group(1))
                int(match.group(2))
                error = float(match.group(3))

                current_solver["iterations"].append(
                    {
                        "iteration": iteration,
                        "error": error,
                        "timestamp": entry["timestamp"],
                    }
                )

        return performance_data

    def find_error_patterns(self) -> dict[str, Any]:
        """Identify common error patterns and their frequency."""
        if not self.parsed:
            self.parse_log_file()

        errors = [e for e in self.entries if e["level"] in ("ERROR", "CRITICAL")]

        if not errors:
            return {"error_patterns": [], "total_errors": 0}

        # Group similar errors
        error_groups = defaultdict(list)

        for error in errors:
            # Normalize error message for grouping
            normalized = re.sub(r"\d+", "N", error["message"])  # Replace numbers
            normalized = re.sub(r"[\d.e-]+", "X", normalized)  # Replace float numbers
            error_groups[normalized].append(error)

        # Sort by frequency
        error_patterns = []
        for pattern, occurrences in sorted(error_groups.items(), key=lambda x: len(x[1]), reverse=True):
            error_patterns.append(
                {
                    "pattern": pattern,
                    "count": len(occurrences),
                    "first_occurrence": occurrences[0]["timestamp"],
                    "last_occurrence": occurrences[-1]["timestamp"],
                    "example_message": occurrences[0]["message"],
                    "locations": list({e.get("location", "Unknown") for e in occurrences}),
                }
            )

        return {
            "error_patterns": error_patterns,
            "total_errors": len(errors),
            "error_timeline": [(e["timestamp"], e["message"]) for e in errors[-10:]],
        }

    def generate_performance_report(self) -> str:
        """Generate a comprehensive performance report."""
        if not self.parsed:
            self.parse_log_file()

        summary = self.get_summary_statistics()
        performance = self.analyze_solver_performance()
        errors = self.find_error_patterns()

        report = []
        report.append("=" * 80)
        report.append("MFG_PDE LOG ANALYSIS REPORT")
        report.append("=" * 80)

        # Summary section
        report.append("\n SUMMARY")
        report.append("-" * 40)
        report.append(f"Log file: {self.log_file_path}")
        report.append(f"Analysis time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Total log entries: {summary.get('total_entries', 0)}")
        report.append(f"Session duration: {summary.get('duration', 'Unknown')}")

        # Level distribution
        if "level_counts" in summary:
            report.append("\n LOG LEVEL DISTRIBUTION")
            for level, count in sorted(summary["level_counts"].items()):
                report.append(f"  {level}: {count} entries")

        # Logger activity
        if "logger_counts" in summary:
            report.append("\n LOGGER ACTIVITY")
            for logger, count in sorted(summary["logger_counts"].items(), key=lambda x: x[1], reverse=True)[:10]:
                report.append(f"  {logger}: {count} entries")

        # Solver performance
        solver_sessions = performance.get("solver_sessions", [])
        if solver_sessions:
            report.append("\n SOLVER PERFORMANCE")
            report.append("-" * 40)
            for session in solver_sessions[-5:]:  # Last 5 sessions
                duration = session.get("duration", 0)
                iterations = len(session.get("iterations", []))
                report.append(
                    f"  {session['solver_name']}: {session['status']} in {duration:.2f}s ({iterations} iterations)"
                )

        # Error analysis
        if errors["total_errors"] > 0:
            report.append("\nERROR: ERROR ANALYSIS")
            report.append("-" * 40)
            report.append(f"Total errors: {errors['total_errors']}")

            for pattern in errors["error_patterns"][:5]:  # Top 5 patterns
                report.append(f"  Pattern (Ã—{pattern['count']}): {pattern['pattern'][:60]}...")

        # Recent warnings
        recent_warnings = summary.get("recent_warnings", [])
        if recent_warnings:
            report.append("\nWARNING:  RECENT WARNINGS")
            report.append("-" * 40)
            for warning in recent_warnings:
                report.append(f"  {warning[:70]}...")

        # Performance timing
        timing_data = performance.get("timing_data", [])
        if timing_data:
            report.append("\nPERFORMANCE TIMING")
            report.append("-" * 40)
            # Group by operation
            timing_by_op = defaultdict(list)
            for timing in timing_data:
                timing_by_op[timing["operation"]].append(timing["duration"])

            for operation, durations in timing_by_op.items():
                avg_time = np.mean(durations)
                total_time = sum(durations)
                count = len(durations)
                report.append(f"  {operation}: {avg_time:.3f}s avg ({total_time:.3f}s total, {count} calls)")

        report.append("\n" + "=" * 80)

        return "\n".join(report)

    def export_analysis_json(self, output_path: str | None = None) -> str:
        """Export analysis results as JSON for automated processing."""
        if not self.parsed:
            self.parse_log_file()

        if output_path is None:
            output_path = str(self.log_file_path.with_suffix(".analysis.json"))

        analysis_data = {
            "log_file": str(self.log_file_path),
            "analysis_timestamp": datetime.now().isoformat(),
            "summary": self.get_summary_statistics(),
            "performance": self.analyze_solver_performance(),
            "error_analysis": self.find_error_patterns(),
        }

        # Convert datetime objects to strings for JSON serialization
        def convert_datetime(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif isinstance(obj, timedelta):
                return str(obj)
            elif isinstance(obj, dict):
                return {k: convert_datetime(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_datetime(item) for item in obj]
            return obj

        analysis_data = convert_datetime(analysis_data)

        # Write JSON file
        with open(output_path, "w") as f:
            json.dump(analysis_data, f, indent=2, default=str)

        self.logger.info(f"Analysis exported to: {output_path}")
        return output_path


# Convenience functions
def analyze_log_file(log_file_path: str, generate_report: bool = True) -> dict[str, Any]:
    """
    Analyze a log file and optionally generate a report.

    Args:
        log_file_path: Path to log file
        generate_report: Whether to print a text report

    Returns:
        Dictionary with analysis results
    """
    analyzer = LogAnalyzer(log_file_path)

    if generate_report:
        report = analyzer.generate_performance_report()
        print(report)

    return {
        "summary": analyzer.get_summary_statistics(),
        "performance": analyzer.analyze_solver_performance(),
        "errors": analyzer.find_error_patterns(),
    }


def analyze_recent_logs(log_directory: str = "logs", days_back: int = 7) -> dict[str, Any]:
    """
    Analyze recent log files from a directory.

    Args:
        log_directory: Directory containing log files
        days_back: How many days back to analyze

    Returns:
        Combined analysis of recent logs
    """
    log_dir = Path(log_directory)
    if not log_dir.exists():
        return {"error": f"Log directory not found: {log_directory}"}

    # Find recent log files
    cutoff_date = datetime.now() - timedelta(days=days_back)
    recent_logs = []

    for log_file in log_dir.glob("*.log"):
        if log_file.stat().st_mtime > cutoff_date.timestamp():
            recent_logs.append(log_file)

    if not recent_logs:
        return {"error": f"No recent log files found in {log_directory}"}

    # Analyze each file
    combined_analysis = {
        "analyzed_files": [],
        "total_entries": 0,
        "combined_errors": [],
        "combined_performance": [],
    }

    for log_file in sorted(recent_logs):
        try:
            analyzer = LogAnalyzer(str(log_file))
            summary = analyzer.get_summary_statistics()
            performance = analyzer.analyze_solver_performance()
            errors = analyzer.find_error_patterns()

            combined_analysis["analyzed_files"].append(
                {
                    "file": str(log_file),
                    "entries": summary.get("total_entries", 0),
                    "errors": errors.get("total_errors", 0),
                    "duration": str(summary.get("duration", "Unknown")),
                }
            )

            combined_analysis["total_entries"] += summary.get("total_entries", 0)
            combined_analysis["combined_errors"].extend(errors.get("error_patterns", []))
            combined_analysis["combined_performance"].extend(performance.get("solver_sessions", []))

        except Exception as e:
            print(f"Error analyzing {log_file}: {e}")

    return combined_analysis


def find_performance_bottlenecks(log_file_path: str, threshold_seconds: float = 1.0) -> list[dict[str, Any]]:
    """
    Find performance bottlenecks in a log file.

    Args:
        log_file_path: Path to log file
        threshold_seconds: Minimum duration to consider a bottleneck

    Returns:
        List of operations that took longer than threshold
    """
    analyzer = LogAnalyzer(log_file_path)
    performance = analyzer.analyze_solver_performance()

    bottlenecks = []

    # Check solver sessions
    for session in performance.get("solver_sessions", []):
        if session.get("duration", 0) > threshold_seconds:
            bottlenecks.append(
                {
                    "type": "solver_session",
                    "operation": f"{session['solver_name']} solve",
                    "duration": session["duration"],
                    "details": session,
                }
            )

    # Check individual operations
    for timing in performance.get("timing_data", []):
        if timing["duration"] > threshold_seconds:
            bottlenecks.append(
                {
                    "type": "operation",
                    "operation": timing["operation"],
                    "duration": timing["duration"],
                    "timestamp": timing["timestamp"],
                }
            )

    return sorted(bottlenecks, key=lambda x: x["duration"], reverse=True)
